{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMHiC4a4Hwnt",
        "outputId": "dc14b9ce-6da3-4604-ed95-b6dacef540f0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mmlt8-dAF28i",
        "outputId": "8535b509-5749-45fe-ace8-86217adbec37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metrics for Multinomial Naive Bayes:\n",
            "Accuracy: 0.9177598596256684\n",
            "F1-Score: 0.27186979840946923\n",
            "Log Loss: 0.3152520985207055\n",
            "Zero-One Loss: 0.08224014037433158\n",
            "Confusion Matrix: [[43200    14]\n",
            " [ 3923   735]]\n",
            "Precision Score: 0.9813084112149533\n",
            "Recall Score: 0.15779304422498927\n",
            "ROC AUC Score: 0.870263676019152\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Metrics for Logistic Regression (Scaled):\n",
            "Accuracy: 0.9279954879679144\n",
            "F1-Score: 0.6342705570291777\n",
            "Log Loss: 0.7320115727709224\n",
            "Zero-One Loss: 0.07200451203208558\n",
            "Confusion Matrix: [[41436  1778]\n",
            " [ 1669  2989]]\n",
            "Precision Score: 0.6270190895741556\n",
            "Recall Score: 0.641691713181623\n",
            "ROC AUC Score: 0.911191897819956\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Metrics for SVM:\n",
            "Accuracy: 0.9572192513368984\n",
            "F1-Score: 0.7370988446726572\n",
            "Log Loss: 0.11721292198248864\n",
            "Zero-One Loss: 0.04278074866310155\n",
            "Confusion Matrix: [[42953   261]\n",
            " [ 1787  2871]]\n",
            "Precision Score: 0.9166666666666666\n",
            "Recall Score: 0.6163589523400601\n",
            "ROC AUC Score: 0.9709648595386461\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, log_loss, zero_one_loss\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_score, recall_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Data Loading (replace with your CSV file path)\n",
        "data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "\n",
        "# Preprocessing with refined stemming\n",
        "def preprocess_text(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [t.lower() for t in tokens]  # Convert to lowercase\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [t for t in tokens if t not in stop_words]  # Remove stop words\n",
        "\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    tokens = [stemmer.stem(t) for t in tokens if len(t) < 20]  # Limit word length\n",
        "\n",
        "    return tokens  # Return the list of tokens\n",
        "\n",
        "# Preprocessing (updated)\n",
        "processed_text = data['comment_text'].apply(preprocess_text)\n",
        "processed_text = processed_text.apply(' '.join)  # Join tokens back into text\n",
        "\n",
        "# Choose your target column (Focus on 'toxic' for this example)\n",
        "target_column = 'toxic'\n",
        "\n",
        "# Feature generation using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "features = vectorizer.fit_transform(processed_text)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, data[target_column], test_size=0.3, random_state=51)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler(with_mean=False)  # explicitly specify with_mean=False\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Function to calculate and print metrics\n",
        "def calculate_metrics(model_name, model, X_test, y_test):\n",
        "    predictions = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    f1 = f1_score(y_test, predictions)\n",
        "    logloss = log_loss(y_test, model.predict_proba(X_test))\n",
        "    zero_one = zero_one_loss(y_test, predictions)\n",
        "    cm = confusion_matrix(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions)\n",
        "    recall = recall_score(y_test, predictions)\n",
        "    fpr, tpr, _ = roc_curve(y_test, model.predict_proba(X_test)[:,1])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    print(f\"\\nMetrics for {model_name}:\")\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"F1-Score:\", f1)\n",
        "    print(\"Log Loss:\", logloss)\n",
        "    print(\"Zero-One Loss:\", zero_one)\n",
        "    print(\"Confusion Matrix:\",cm)\n",
        "    print(\"Precision Score:\", precision)\n",
        "    print(\"Recall Score:\", recall)\n",
        "    print(\"ROC AUC Score:\", roc_auc)\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "# Model 1: Multinomial Naive Bayes\n",
        "model_nb = MultinomialNB()\n",
        "model_nb.fit(X_train, y_train)\n",
        "\n",
        "# Model 2: Logistic Regression with increased max_iter and scaled data\n",
        "model_lr = LogisticRegression(random_state=51, max_iter=1000) # Increase max_iter\n",
        "model_lr.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Model 3: Support Vector Machine (SVM) with probability estimates enabled\n",
        "model_svm = SVC(random_state=51, probability=True)  # Enable probability estimates\n",
        "model_svm.fit(X_train, y_train)\n",
        "\n",
        "calculate_metrics(\"Multinomial Naive Bayes\", model_nb, X_test, y_test)\n",
        "calculate_metrics(\"Logistic Regression (Scaled)\", model_lr, X_test_scaled, y_test)\n",
        "calculate_metrics(\"SVM\", model_svm, X_test, y_test)"
      ]
    }
  ]
}